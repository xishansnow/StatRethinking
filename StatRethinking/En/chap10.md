# 10 Big Entropy and the Generalized Linear Model 

Most readers of this book will share the experience of fighting with tangled electrical cords. Whether behind a desk or stuffed in a box, cords and cables tend toward tying themselves in knots. Why is this? There is of course real physics at work. But at a descriptive level, the reason is entropy: There are vastly more ways for cords to end up in a knot than for them to remain untied.^160 So if I were to carefully lay a dozen cords in a box and then seal the box and shake it, we should bet that at least some of the cords will be tangled together when I again open the box. We don’t need to know anything about the physics of cords or knots.

We just have to bet on entropy. Events that can happen vastly more ways are more likely.

Exploiting entropy is not going to untie your cords. But it will help you solve some problems in choosing distributions. Statistical models force many choices upon us. Some of these choices are distributions that represent uncertainty. We must choose, for each parameter, a prior distribution. And we must choose a likelihood function, which serves as a distribution of data. There are conventional choices, such as wide Gaussian priors and the Gaussian likelihood of linear regression. These conventional choices work unreasonably well in many circumstances. But very often the conventional choices are not the best choices. Inference can be more powerful when we use all of the information, and doing so usually requires going beyond convention.

To go beyond convention, it helps to have some principles to guide choice. When an engineer wants to make an unconventional bridge, engineering principles help guide choice.

When a researcher wants to build an unconventional model, entropy provides one useful principle to guide choice of probability distributions: Bet on the distribution with the biggest entropy. Why? There are three sorts of justifications.

First, the distribution with the biggest entropy is the widest and least informative distribution. Choosing the distribution with the largest entropy means spreading probability as evenly as possible, while still remaining consistent with anything we think we know about a process. In the context of choosing a prior, it means choosing the least informative distribution consistent with any partial scientific knowledge we have about a parameter. In the context of choosing a likelihood, it means selecting the distribution we’d get by counting up all the ways outcomes could arise, consistent with the constraints on the outcome variable.
In both cases, the resulting distribution embodies the least information while remaining true to the information we’ve provided.

Second, nature tends to produce empirical distributions that have high entropy. Back in Chapter 4, I introduced the Gaussian distribution by demonstrating how any process that repeatedly adds together fluctuations will tend towards an empirical distribution with the distinctive Gaussian shape. That shape is the one that contains no information about the underlying process except its location and variance. As a result, it has maximum entropy.Natural processes other than addition also tend to produce maximum entropy distributions.But they are not Gaussian. They retain different information about the underlying process.

Third, regardless of why it works, it tends to work. Mathematical procedures are effective even when we don’t understand them. There are no guarantees that any logic in the small world (Chapter2) will be useful in the large world. We use logic in science because it has a strong record of effectiveness in addressing real world problems. This is the historical justification: The approach has solved difficult problems in the past. This is no guarantee that it will work on your problem. But no approach can guarantee that.

This chapter serves as a conceptual introduction to **generalized linear models** and the principle of **maximum entropy**. A generalized linear model (GLM) is much like the linear regressions of previous chapters. It is a model that replaces a parameter of a likelihood function with a linear model. But GLMs need not use Gaussian likelihoods. Any likelihood function can be used, and linear models can be attached to any or all of the parameters that describe its shape. The principle of maximum entropy helps us choose likelihood functions, by providing a way to use stated assumptions about constraints on the outcome variable to choose the likelihood function that is the most conservative distribution compatible with the known constraints. Using this principle recovers all the most common likelihood functions of many statistical approaches, Bayesian or not, while simultaneously providing a clear rationale for choice among them.

The chapters to follow this one build computational skills for working with different flavors of GLM.Chapter 11addresses models for count variables.Chapter 12explores more complicated models, such as ordinal outcomes and mixtures. Portions of these chapters are specialized by model type. So you can skip sections that don’t interest you at the moment.
The multilevel chapters, beginning withChapter 13, make use of binomial count models, however. So some familiarity with the material inChapter 11will be helpful.

 **Rethinking: Bayesian updating is entropy maximization.** Another kind of probability distribution, the posterior distribution deduced by Bayesian updating, is also a case of maximizing entropy. The posterior distribution has the greatest entropy relative to the prior (the smallest cross entropy) among all distributions consistent with the assumed constraints and the observed data.^161 This fact won’t change how you calculate. But it should provide a deeper appreciation of the fundamental connections between Bayesian inference and information theory. Notably, Bayesian updating is just like maximum entropy in that it produces the least informative distribution that is still consistent with our assumptions. Or you might say that the posterior distribution has the smallest divergence from the prior that is possible while remaining consistent with the constraints and data.

## 10.1. Maximum entropy 

In Chapter 7, you met the basics of information theory. In brief, we seek a measure of uncertainty that satisfies three criteria: (1) the measure should be continuous; (2) it should increase as the number of possible events increases; and (3) it should be additive. The resulting unique measure of the uncertainty of a probability distribution p with probabilities pi for each possible event i turns out to be just the average log-probability: 



 This function is known as information entropy.
 

 The principle of maximum entropy applies this measure of uncertainty to the problem of choosing among probability distributions. Perhaps the simplest way to state the maximum entropy principle is: The distribution that can happen the most ways is also the distribution with the biggest information entropy. The distribution with the biggest entropy is the most conservative distribution that obeys its constraints.

There’s nothing intuitive about this idea, so if it seems weird, you are normal.

To begin to understand maximum entropy, forget about information and probability theory for the moment. Imagine instead 5 buckets and a pile of 10 individually numbered pebbles. You stand and toss all 10 pebbles such that each pebble is equally likely to land in any of the 5 buckets. This means that every particular arrangement of the 10 individual pebbles is equally likely—it’s just as likely to get all 10 in bucket 3 as it is to get pebble 1 in bucket 2, pebbles 2–9 in bucket 3, and pebble 10 in bucket 4.

But some kinds of arrangements are much more likely. Some arrangements look the same, because they show the same number of pebbles in the same individual buckets. These are distributions of pebbles. Figure 10.1illustrates 5 such distributions. So for example there is only 1 way to arrange the individual pebbles so that all of them are in bucket 3 (plot A). But there are 90 ways to arrange the individual pebbles so that 2 of them are in bucket 2, 8 in bucket 3, and 2 in bucket 4 (plot B). Plots C, D, and E show that the number of unique arrangements corresponding to a distribution grows very rapidly as the distribution places a more equal number of pebbles in each bucket. By the time there are 2 pebbles in each bucket (plot E), there are 113400 ways to realize this distribution. There is no other distribution of the pebbles that can be realized a greater number of ways.

Let’s put each distribution of pebbles in a list: 


And let’s normalize each such that it is a probability distribution. This means we just divide each count of pebbles by the total number of pebbles: 

 (^11121212323234343454545551112121232323434345454555) (^1112121232323434345454555) 0.0 0.2 0.4 0.6 0.8 1.0 1.2 0.0 0.5 1.0 1.5 log(ways) per pebble entropy A B C D E Figure 10.1. Entropy as a measure of the number of unique arrangements of a system that produce the same distribution. Plots A through E show the numbers of unique ways to arrange 10 pebbles into each of 5 different distributions. Bottom-right: The entropy of each distribution plotted against the log number of ways per pebble to produce it.
each distribution can be realized, then divide that logarithm by 10, the number of pebbles.
This gives us the log ways per pebble for each distribution: R code 10.4 ways <c(1,90,1260,37800,113400) logwayspp <log(ways)/10 The bottom-right plot inFigure 10.1displays theselogwaysppvalues against the information entropiesH. These two sets of values contain the same information, as information entropy is an approximation of the log ways per pebble (see the Overthinking box at the end for details). As the number of pebbles grows larger, the approximation gets better. It’s 

 
 already extremely good, for just 10 pebbles. Information entropy is a way of counting how many unique arrangements correspond to a distribution.
This is useful, because the distribution that can happen the greatest number of ways is the most plausible distribution. Call this distribution the **maximum entropy distribution**.

As you might guess from the pebble example, the number of ways corresponding to the maximum entropy distribution eclipses that of any other distribution. And the numbers of ways for each distribution most similar to the maximum entropy distribution eclipse those of less similar distributions. And so on, such that the vast majority of unique arrangements of pebbles produce either the maximum entropy distribution or rather a distribution very similar to it. And that is why it’s often effective to bet on maximum entropy: It’s the center of gravity for the highly plausible distributions.
Its high plausibility is conditional on our assumptions, of course. To grasp the role of assumptions—constraints and data—in maximum entropy, we’ll explore two examples.

First, we’ll derive the Gaussian distribution as the solution to an entropy maximization problem. Second, we’ll derive the binomial distribution, which we used way back inChapter 2 to draw marbles and toss globes, as the solution to a different entropy maximization problem. These derivations will not be mathematically rigorous. Rather, they will be graphical and aim to deliver a conceptual appreciation for what this thing calledentropyis doing. The Overthinking boxes in this section provide connections to the mathematics, for those who are interested.

But the most important thing is to be patient with yourself. Understanding of and intuition for probability theory comes with experience. You can usefully apply the principle of maximum entropy before you fully understand it. Indeed, it may be that no one fully understands it. Over time, and within the contexts that you find it useful, the principle will become more intuitive.

 **Rethinking: What good is intuition?** Like many aspects of information theory, maximum entropy is not very intuitive. But note that intuition is just a guide to developing methods. When a method works, it hardly matters whether our intuition agrees. This point is important, because some people still debate statistical approaches on the basis of philosophical principles and intuitive appeal. Philosophy does matter, because it influences development and application. But it is a poor way to judge whether or not an approach is useful. Results are what matter. For example, the three criteria used to derive information entropy, back inChapter 7, are not also the justification for using information entropy. The justification is rather that it has worked so well on so many problems where other methods have failed.

 ``` Overthinking: The Wallis derivation. Intuitively, we can justify maximum entropy just based upon the definition of information entropy. But there’s another derivation, attributed to Graham Wallis,^162 that doesn’t invoke “information” at all. Here’s a short version of the argument. Suppose there areM observable events, and we wish to assign a plausibility to each. We know some constraints about the process that produces these events, such as its expected value or variance. Now imagine setting upM buckets and tossing a large numberNof individual stones into them at random, in such a way that each stone is equally likely to land in any of theMbuckets. After all the stones have landed, we count up the number of stones in each bucketiand use these countsnito construct a candidate probability distribution defined bypi=ni/N. If this candidate distribution is consistent with our constraints, we add it to a list. If not, we empty the buckets and try again. After many rounds of this, the distribution that has occurred the most times is the fairest—in the sense that no bias was involved in tossing the stones into buckets—that still obeys the constraints that we imposed.
``` 
 
If we could employ the population of a large country in tossing stones every day for years on end, we could do this empirically. Luckily, the procedure can be studied mathematically. The probability of any particular candidate distribution is just its multinomial probability, the probability of the observed stone counts under uniform chances of landing in each bucket: 

 The distribution that is realized most often will have the largest value of that ugly fractionWwith the factorials in it. CallWthemultiplicity, because it states the number of different ways a particular set of counts could be realized. For example, landing all stones in the first bucket can happen only one way, by getting all the stones into that bucket and none in any of the other buckets. But there are many more ways to evenly distribute the stones in the buckets, because order does not matter. We care about this multiplicity, because we are seeking the distribution that would happen most often.
So by selecting the distribution that maximizes this multiplicity, we can accomplish that goal.
We’re almost at entropy. It’s easier to work with^1 Nlog(W), which will be maximized by the same distribution asW. Also note thatni=Npi. These changes give us: 1 N 

 
logN!− 
 Now sinceNis very large, we can approximate logN!with Stirling’s approximation,NlogN−N: 

 And that’s the exact same formula as Shannon’s information entropy. Among distributions that satisfy our constraints, the distribution that maximizes the expression above is the distribution that spreads out probability as evenly as possible, while still obeying the constraints.
This result generalizes easily to the case in which there is not an equal chance of each stone landing in each bucket.^163 If we have prior information specified as a probabilityqithat a stone lands in bucketi, then the quantity to maximize is instead: 1 N 

 You may recognize this as KL divergence fromChapter 7, just with a negative in front. This reveals that the distribution that maximizes entropy is also the distribution that minimizes the information distance from the prior, among distributions consistent with the constraints. When the prior is flat, maximum entropy gives the flattest distribution possible. When the prior is not flat, maximum entropy updates the prior and returns the distribution that is most like the prior but still consistent with the constraints. This procedure is often calledminimum cross-entropy. Furthermore, Bayesian updating itself can be expressed as the solution to a maximum entropy problem in which the data represent constraints.^164 Therefore Bayesian inference can be seen as producing a posterior distribution that is most similar to the prior distribution as possible, while remaining logically consistent with the stated information.
 **10.1.1. Gaussian.** When I introduced the Gaussian distribution inChapter 4(page 72), it emerged from a generative process in which 1000 people repeatedly flipped coins and took steps left (heads) or right (tails) with each flip. The addition of steps led inevitably to a distribution of positions resembling the Gaussian bell curve. This process represents the most basic generative dynamic that leads to Gaussian distributions in nature. When many small factors add up, the ensemble of sums tends towards Gaussian.
But obviously many other distributions are possible. The coin-flipping dynamic could place all 1000 people on the same side of the soccer field, for example. So why don’t we see 
 
 Figure 10.2. Maximum entropy and the Gaussian distribution. Left: Comparison of Gaussian (blue) and several other continuous distributions with the same variance. Right: Entropy is maximized when curvature of a generalized normal distribution matches the Gaussian, where shape is equal to 2.
``` those other distributions in nature? Because for every sequence of coin flips that can produce such an imbalanced outcome, there are vastly many more that can produce an approximately balanced outcome. The bell curve emerges, empirically, because there are so many different detailed states of the physical system that can produce it. Whatever does happen, it’s bound to produce an ensemble that is approximately Gaussian. So if all you know about a collection of continuous values is its variance (or that it has a finite variance, even if you don’t know it yet), the safest bet is that the collection ends up in one of these vastly many bell-shaped configurations.^165 And maximum entropy just seeks the distribution that can arise the largest number of ways, so it does a good job of finding limiting distributions like this. But since entropy is maximized when probability is spread out as evenly as possible, maximum entropy also seeks the distribution that is most even, while still obeying its constraints. In order to visualize how the Gaussian is the most even distribution for any given variance, let’s consider a family of generalized distributions with equal variance. Ageneralized normal distributionis defined by the probability density: 

 We want to compare a regular Gaussian distribution with varianceσ^2 to several generalized normals with the same variance.^166 The left-hand plot inFigure 10.2presents one Gaussian distribution, in blue, together with three generalized normal distributions with the same variance. All four distributions have varianceσ^2 = 1. Two of the generalized distributions are more peaked, and have thicker tails, than the Gaussian. Probability has been redistributed from the middle to the tails, keeping the variance constant. The third generalized distribution is instead thicker 
  
in the middle and thinner in the tails. It again keeps the variance constant, this time by redistributing probability from the tails to the center. The blue Gaussian distribution sits between these extremes.
In the right-hand plot ofFigure 10.2,βis called “shape” and varies from 1 to 4, and entropy is plotted on the vertical axis. The generalized normal is perfectly Gaussian where β=2, and that’s exactly where entropy is maximized. All of these distributions are symmetrical, but that doesn’t affect the result. There are other generalized families of distributions that can be skewed as well, and even then the bell curve has maximum entropy. See the Overthinking box at the bottom of this page, if you want a more satisfying proof.
To appreciate why the Gaussian shape has the biggest entropy for any continuous distribution with this variance, consider that entropy increases as we make a distribution flatter.
So we could easily make up a probability distribution with larger entropy than the blue distribution inFigure 10.2: Just take probability from the center and put it in the tails. The more uniform the distribution looks, the higher its entropy will be. But there are limits on how much of this we can do and maintain the same variance,σ^2 =1. A perfectly uniform distribution would have infinite variance, in fact. So the variance constraint is actually a severe constraint, forcing the high-probability portion of the distribution to a small area around the mean. Then the Gaussian distribution gets its shape by being as spread out as possible for a distribution with fixed variance.
The take-home lesson from all of this is that, if all we are willing to assume about a collection of measurements is that they have a finite variance, then the Gaussian distribution represents the most conservative probability distribution to assign to those measurements.
But very often we are comfortable assuming something more. And in those cases, provided our assumptions are good ones, the principle of maximum entropy leads to distributions other than the Gaussian.
 **Overthinking: Proof of Gaussian maximum entropy.** Proving that the Gaussian has the largest entropy of any distribution with a given variance is easier than you might think. Here’s the shortest proof I know.^167 Letp(x)=( 2 πσ^2 )−^1 /^2 exp(−(x−μ)^2 /( 2 σ^2 ))stand for the Gaussian probability density function. Letq(x)be some other probability density function with the same varianceσ^2. The meanμdoesn’t matter here, because entropy doesn’t depend upon location, just shape.
The entropy of the Gaussian isH(p)=− 
R p(x)logp(x)dx=^12 log( 2 πeσ^2 ). We seek to prove that no distributionq(x)can have higher entropy than this, provided they have the same variance and are both defined on the entire real number line, from−∞to+∞. We can accomplish this by using our old friend, fromChapter 7, KL divergence: 

 H(q)=− 

 R q(x)logp(x)dxis the crossentropy of the two. Why useDKLhere? Because it is always positive (or zero), which guarantees that −H(q,p)≥H(q). So while we can’t computeH(q), it turns out that we can computeH(q,p). And as you’ll see, that solves the whole problem. So let’s computeH(q,p). It’s defined as: 

 This will be conceptually easier if we remember that the integral above just takes the average overx.
So we can rewrite the above as: 

 
(x−μ)^2 
 

 
log( 2 πσ^2 )+ 1 
 And that is exactly−H(p). So since−H(q,p)≥H(q)by definition, and sinceH(p)=−H(q,p), it follows thatH(p) ≥H(q). The Gaussian has the highest entropy possible for any continuous distribution with varianceσ^2.
 
 For the moment, we’ll work with this elementary form, because it will make it easier to appreciate the basis for treating all sequences with the same countyas the same outcome.
Now we want to demonstrate that this same distribution has the largest entropy of any distribution that satisfies these constraints: (1) only two unordered events, and (2) constant expected value. To develop some intuition for the result, let’s explore two examples in which we fix the expected value. In both examples, we have to assign probability to each possible outcome, while keeping the expected value of the distribution constant. And in both examples, the unique distribution that maximizes entropy is the binomial distribution with the same expected value.
Here’s the first example. Suppose again, like inChapter 2, that we have a bag with an unknown number of blue and white marbles within it. We draw two marbles from the bag, with replacement. There are therefore four possible sequences: (1) two white marbles, (2) one blue and then one white, (3) one white and then one blue, and (4) two blue marbles.
Our task is to assign probabilities to each of these possible outcomes. Suppose we know that the expected number of blue marbles over two draws is exactly 1. This is the expected value constraint on the distributions we’ll consider.
We seek the distribution with the biggest entropy. Let’s consider four candidate distributions, shown inFigure 10.3. Here are the probabilities that define each distribution: Distribution ww bw wb bb A 1/4 1/4 1/4 1/4 B 2/6 1/6 1/6 2/6 C 1/6 2/6 2/6 1/6 D 1/8 4/8 2/8 1/8 
 
 A B 
C D 
``` Figure 10.3. Four different distributions with the same expected value, 1 blue marble in 2 draws. The outcomes on the horizontal axes correspond to 2 white marbles (ww), 1 blue and then 1 white (bw), 1 white and then 1 blue (wb), and 2 blue marbles (bb).

 Distribution A is the binomial distribution withn= 2 andp = 0 .5. The outcomes bw and wb are usually collapsed into the same outcome type. But in principle they are different outcomes, whether we care about the order of outcomes or not. So the corresponding binomial probabilities are Pr(ww)=( 1 −p)^2 , Pr(bw)=p( 1 −p), Pr(wb)=( 1 −p)p, and Pr(bb)=p^2. Sincep= 0 .5 in this example, all four probabilities evaluate to 1/4.
The other distributions—B, C, and D—have the same expected value, but none of them is binomial. We can expediently verify this by placing them inside alistand passing each to an expected value formula: ``` R code 10.5 # build list of the candidate distributions p <list() p[[1]] <c(1/4,1/4,1/4,1/4) p[[2]] <c(2/6,1/6,1/6,2/6) p[[3]] <c(1/6,2/6,2/6,1/6) p[[4]] <c(1/8,4/8,2/8,1/8) 

 R code 10.6 # compute entropy of each distribution sapply( p , function(p) -sum( p*log(p) ) ) 
``` [1] 1.386294 1.329661 1.329661 1.213008 Distribution A, the binomial distribution, has the largest entropy among the four. To appreciate why, consider that information entropy increases as a probability distribution becomes more even. Distribution A is a flat line, as you can see inFigure 10.3. It can’t be made any more even, and each of the other distributions is clearly less even. That’s why they have smaller entropies. And since distribution A is consistent with the constraint that the expected value be 1, it follows that distribution A, which is binomial, has the maximum entropy of any distribution with these constraints.
``` 

 This example is too special to demonstrate the general case, however. It’s special because when the expected value is 1, the distribution over outcomes can be flat and remain consistent with the constraint. But what about when the expected value constraint is not 1? Suppose for our second example that the expected value must be instead 1.4 blue marbles in two draws.
This corresponds top= 0 .7. So you can think of this as 7 blue marbles and 3 white marbles hidden inside the bag. The binomial distribution with this expected value is: R code p <0.7 10.7 ( A <c( (1-p)^2 , p*(1-p) , (1-p)*p , p^2 ) ) 

 [1] 1.221729 So if we randomly generate thousands of distributions with expected value 1.4, we expect that none will have a larger entropy than this.
We can use a short R function to simulate random probability distributions that have any specified expected value. The code below will do the job. Don’t worry about how it works (unless you want to^168 ).
 R code sim.p <function(G=1.4) { 10.9 x123 <runif(3) x4 <( (G)*sum(x123)-x123[2]-x123[3] )/(2-G) z <sum( c(x123,x4) ) p <c( x123 , x4 )/z list( H=-sum( p*log(p) ) , p=p ) } 
``` This function generates a random distribution with expected valueGand then returns its entropy along with the distribution. We want to invoke this function a large number of times.
Here is how to call it 100000 times and then plot the distribution of resulting entropies: R code H <replicate( 1e5 , sim.p(1.4) ) 10.10 dens( as.numeric(H[1,]) , adj=0.1 ) 
 The listHnow holds 100,000 distributions and their entropies. The distribution of entropies is shown in the left-hand plot inFigure 10.4. The letters A, B, C, and D mark different example entropies. The distributions corresponding to each are shown in the right-hand part of the figure. The distribution A with the largest observed entropy is nearly identical to the binomial we calculated earlier. And its entropy is nearly identical as well.
You don’t have to take my word for it. Let’s split out the entropies and distributions, so that it’s easier to work with them: ``` 

 ### A 
### B 
### D C 
### A B 
### C D 
``` Figure 10.4. Left: Distribution of entropies from randomly simulated distributions with expected value 1.4. The letters A, B, C, and D mark the entropies of individual distributions shown on the right. Right: Individual probability distributions. As entropy decreases, going from A to D, the distribution becomes more uneven. The distribution marked A is the binomial distribution withnp= 1 .4.
``` R code 10.11 entropies <as.numeric(H[1,]) distributions <H[2,] 

 R code 10.12 max(entropies) 
``` [1] 1.221728 That value is nearly identical to the entropy of the binomial distribution we calculated before.
And the distribution with that entropy is: ``` R code 10.13 distributions[ which.max(entropies) ] 
``` [[1]] [1] 0.08981599 0.21043116 0.20993686 0.48981599 And that’s almost exactly{ 0. 09 , 0. 21 , 0. 21 , 0. 49 }, the distribution we calculated earlier.
The other distributions inFigure 10.4—B, C, and D—are all less even than A. They demonstrate how as entropy declines the probability distributions become progressively less even. All four of these distributions really do have expected value 1.4. But among the infinite distributions that satisfy this constraint, it is only the most even distribution, the exact one nominated by the binomial distribution, that has greatest entropy.
So what? There are a few conceptual lessons to take away from this example. First, hopefully it reinforces the maximum entropy nature of the binomial distribution. When only two un-ordered outcomes are possible—such as blue and white marbles—and the expected ``` 

 numbers of each type of event are assumed to be constant, then the distribution that is most consistent with these constraints is the binomial distribution. This distribution spreads probability out as evenly and conservatively as possible.
Second, of course usually we do not know the expected value, but wish to estimate it. But this is actually the same problem, because assuming the distribution has a constant expected value leads to the binomial distribution as well, but with unknown expected valuenp, which must be estimated from the data. (You’ll learn how to do this inChapter 11.) If only two un-ordered outcomes are possible and you think the process generating them is invariant in time—so that the expected value remains constant at each combination of predictor values— then the distribution that is most conservative is the binomial. This is analogous to how the Gaussian distribution is the most conservative distribution for a continuous outcome variable with finite variance. Variables with different constraints get different maximum entropy distributions, but the underlying principle remains the same.
Third, back inChapter 2, we derived the binomial distribution just by counting how many paths through the garden of forking data were consistent with our assumptions. For each possible composition of the bag of marbles—which corresponds here to each possible expected value—there is a unique number of ways to realize any possible sequence of data.
The likelihoods derived in that way turn out to be exactly the same as the likelihoods we get by maximizing entropy. This is not a coincidence. Entropy counts up the number of different ways a process can produce a particular result, according to our assumptions. The garden of forking data did only the same thing—count up the numbers of ways a sequence could arise, given assumptions.
Entropy maximization, like so much in probability theory, is really just counting. But it’s abbreviated counting that allows us to generalize lessons learned in one context to new problems in new contexts. Instead of having to tediously draw out a garden of forking data, we can instead map constraints on an outcome to a probability distribution. There is no guarantee that this is the best probability distribution for the real problem you are analyzing. But there is a guarantee that no other distribution more conservatively reflects your assumptions.
That’s not everything, but nor is it nothing. Any other distribution implies hidden constraints that are unknown to us, reflecting phantom assumptions. A full and honest accounting of assumptions is helpful, because it aids in understanding how a model misbehaves.
And since all models misbehave sometimes, it’s good to be able to anticipate those times before they happen, as well as to learn from those times when they inevitably do.
 **Rethinking: Conditional independence.** All this talk of constant expected value brings up an important question: Do these distributions necessarily assume that each observation is uncorrelated with every other observation? Not really. What is usually meant by “independence” in a probability distribution is just that each observation is uncorrelated with the others, once we know the corresponding predictor values. This is usually known as **conditional independence** , the claim that observations are independent after accounting for differences in predictors, through the model. It’s a modeling assumption. What this assumption doesn’t cover is a situation in which an observed event directly causes the next observed event. For example, if you buy the next Nick Cave album because I buy the next Nick Cave album, then your behavior is not independent of mine, even after conditioning on the fact that we both like that sort of music.
 

 Overthinking: Binomial maximum entropy. The usual way to derive a maximum entropy distribution is to state the constraints and then use a mathematical device called theLagrangianto solve for the probability assignments that maximize entropy. But instead we’ll extend the strategy used in the Overthinking box onpage 306. As a bonus, this strategy will allow us to derive the constraints that are necessary for a distribution, in this case the binomial, to be a maximum entropy distribution.
Letpbe the binomial distribution, and letpibe the probability of a sequence of observationsi with number of successesxiand number of failuresn−xi. Letqbe some other discrete distribution defined over the same set of observable sequences. As before, KL divergence tells us that: −H(q,p)≥H(q)=⇒ − 
 What we’re going to do now is work withH(q,p)and simplify it until we can isolate the constraint that defines the class of distributions for whichphas maximum entropy. Letλ= 

 qixi | {z } ̄q The term on the far right labeled ̄qis the expected value of the distributionq. If we knew it, we could complete the calculation, because no other term depends uponqi. This means that expected value is the constraint that defines the class of distributions for which the binomialphas maximum entropy.
If we now set the expected value ofqequal toλ, thenH(q)=H(p). For any other expected value of q,H(p)>H(q).
Finally, notice the term log[λ/(n−λ)]. This term is the log of the ratio of the expected number of successes to the expected number of failures. That ratio is the “odds” of a success, and its logarithm is called “log odds.” This quantity will feature prominently in models we construct from the binomial distribution, inChapter 11.
``` ### 10.2. Generalized linear models 

 For an outcome variable that is continuous and far from any theoretical maximum or minimum, this sort of Gaussian model has maximum entropy.
But when the outcome variable is either discrete or bounded, a Gaussian likelihood is not the most powerful choice. Consider for example a count outcome, such as the number of blue marbles pulled from a bag. Such a variable is constrained to be zero or a positive integer. Using a Gaussian model with such a variable won’t result in a terrifying explosion.
But it can’t be trusted to do much more than estimate the average count. It certainly can’t be trusted to produce sensible predictions, because while you and I know that counts can’t ``` 

 Figure 10.5. Why we need link functions.
The solid blue line is a linear model of a probability mass. It increases linearly with a predictor,x, on the horizontal axis. But when it reaches the maximum probability mass of 1, at the dashed boundary, it will happily continue upwards, as shown by the dashed blue line. In reality, further increases inxcould not further increase probability, as indicated by the horizontal continuation of the solid trend.
``` be negative, a linear regression model does not. So it would happily predict negative values, whenever the mean count is close to zero.
Luckily, it’s easy to do better. By using all of our prior knowledge about the outcome variable, usually in the form of constraints on the possible values it can take, we can appeal to maximum entropy for the choice of distribution. Then all we have to do is generalize the linear regression strategy—replace a parameter describing the shape of the likelihood with a linear model—to probability distributions other than the Gaussian.
This is the essence of a **generalized linear model**.^169 And it results in models that look like this: 

 There are only two changes here from the familiar Gaussian model. The first is principled— the principle of maximum entropy. The second is an epicycle—a modeling trick that works descriptively but not causally—but a quite successful one. I’ll briefly explain each, before moving on in the remainder of the section to describe all of the most common distributions used to construct generalized linear models. Later chapters show you how to implement them.
First, the likelihood is binomial instead of Gaussian. For a count outcomeyfor which each observation arises fromntrials and with constant expected valuenp, the binomial distribution has maximum entropy. So it’s the least informative distribution that satisfies our prior knowledge of the outcomesy. If the outcome variable had different constraints, it could be a different maximum entropy distribution.
Second, there is now a funny littlefat the start of the second line of the model. This represents a **link function** , to be determined separately from the choice of distribution.
Generalized linear models need a link function, because rarely is there a “μ”, a parameter describing the average outcome, and rarely are parameters unbounded in both directions, likeμis. For example, the shape of the binomial distribution is determined, like the Gaussian, by two parameters. But unlike the Gaussian, neither of these parameters is the mean. Instead, the mean outcome isnp, which is a function of both parameters. Sincenis usually known (but not always), it is most common to attach a linear model to the unknown part,p. Butpis 
  
a probability mass, sopimust lie between zero and one. But there’s nothing to stop the linear modelα+βxifrom falling below zero or exceeding one.Figure 10.5plots an example. The link functionfprovides a solution to this common problem. This chapter will introduce the two most common link functions. You’ll see how to use them in the chapters that follow.
 **Rethinking: The scourge of Histomancy.** One strategy for choosing an outcome distribution is to plot the histogram of the outcome variable and, by gazing into its soul, decide what sort of distribution function to use. Call this strategy **Histomancy** , the ancient art of divining likelihood functions from empirical histograms. This sorcery is used, for example, when testing for normality before deciding whether or not to use a non-parametric procedure. Histomancy is a false god, because even perfectly good Gaussian variables may not look Gaussian when displayed as a histogram. Why? Because at most what a Gaussian likelihood assumes is not that the aggregated data look Gaussian, but rather that theresiduals, after fitting the model, look Gaussian. So for example the combined histogram of male and female body weights is certainly not Gaussian. But it is (approximately) a mixture of Gaussian distributions. So after conditioning on sex, the residuals may be quite normal. Other times, people decide not to use a Poisson model, because the variance of the aggregate outcome exceeds its mean (seeChapter 11). But again, at most what a Poisson likelihood assumes is that the variance equals the mean after conditioning on predictors. It may very well be that a Gaussian or Poisson likelihood is a poor assumption in any particular context. But this can’t easily be decided via Histomancy. This is why we need principles, whether maximum entropy or otherwise.
 **10.2.1. Meet the family.** The most common distributions used in statistical modeling are members of a family known as the **exponential family**. Every member of this family is a maximum entropy distribution, for some set of constraints. And conveniently, just about every other statistical modeling tradition employs the exact same distributions, even though they arrive at them via justifications other than maximum entropy.
Figure 10.6illustrates the representative shapes of the most common exponential family distributions used in GLMs. The horizontal axis in each plot represents values of a variable, and the vertical axis represents probability density (for the continuous distributions) or probability mass (for the discrete distributions). For each distribution, the figure also provides the notation (above each density plot) and the name of R’s corresponding built-in distribution function (below each density plot). The gray arrows inFigure 10.6indicate some of the ways that these distributions are dynamically related to one another. These relationships arise from generative processes that can convert one distribution to another. You do not need to know these relationships in order to successfully use these distributions in your modeling. But the generative relationships do help to demystify these distributions, by tying them to causation and measurement.
Two of these distributions, the Gaussian and binomial, are already familiar to you. Together, they comprise the most commonly used outcome distributions in applied statistics, through the procedures of linear regression (Chapter 4) and logistic regression (Chapter 11).
There are also three new distributions that deserve some commentary.
The **exponential distribution** (center) is constrained to be zero or positive. It is a fundamental distribution of distance and duration, kinds of measurements that represent displacement from some point of reference, either in time or space. If the probability of an event is constant in time or across space, then the distribution of events tends towards exponential. The exponential distribution has maximum entropy among all non-negative continuous distributions with the same average displacement. Its shape is described by a 
 
 Figure 10.6. Some of the exponential family distributions, their notation, and some of their relationships. Center: exponential distribution. Clockwise, from top-left: gamma, normal (Gaussian), binomial and Poisson distributions.
``` single parameter, the rate of eventsλ, or the average displacementλ−^1. This distribution is the core of survival and event history analysis, which is not covered in this book.
The **gamma distribution** (top-left) is also constrained to be zero or positive. It too is a fundamental distribution of distance and duration. But unlike the exponential distribution, the gamma distribution can have a peak above zero. If an event can only happen after two or more exponentially distributed events happen, the resulting waiting times will be gamma distributed. For example, age of cancer onset is approximately gamma distributed, since multiple events are necessary for onset.^170 The gamma distribution has maximum entropy among all distributions with the same mean and same average logarithm. Its shape is described by two parameters, but there are at least three different common descriptions of these parameters, so some care is required when working with it. The gamma distribution is common in survival and event history analysis, as well as some contexts in which a continuous measurement is constrained to be positive.
The **Poisson distribution** (bottom-left) is a count distribution like the binomial. It is actually a special case of the binomial, mathematically. If the number of trialsnis very large (and usually unknown) and the probability of a successpis very small, then a binomial distribution converges to a Poisson distribution with an expected rate of events per unit time ofλ=np. Practically, the Poisson distribution is used for counts that never get close to any 
  
theoretical maximum. As a special case of the binomial, it has maximum entropy under exactly the same constraints. Its shape is described by a single parameter, the rate of events λ. Poisson GLMs are detailed in the next chapter.
There are many other exponential family distributions, and many of them are useful.
But don’t worry that you need to memorize them all. You can pick up new distributions, and the sorts of generative processes they correspond to, as needed. It’s also not important that an outcome distribution be a member of the exponential family—if you think you have good reasons to use some other distribution, then use it. But you should also check its performance, just like you would any modeling assumption.
 **Rethinking: A likelihood is a prior.** In traditional statistics, likelihood functions are “objective” and prior distributions “subjective.” In Bayesian statistics, likelihoods are deeply related to prior probability distributions: They are priors for the data, conditional on the parameters. And just like with other priors, there is no correct likelihood. But there are better and worse likelihoods, depending upon the context. Useful inference does not require that the data (or residuals) be actually distributed according to the likelihood anymore than it requires the posterior distribution to be like the prior. The duality between likelihoods and priors will become quite explicit inChapter 15.
 **10.2.2. Linking linear models to distributions.** To build a regression model from any of the exponential family distributions is just a matter of attaching one or more linear models to one or more of the parameters that describe the distribution’s shape. But as hinted at earlier, usually we require a **link function** to prevent mathematical accidents like negative distances or probability masses that exceed 1. So for any outcome distribution, say for example the exotic “Zaphod” distribution,^171 we write: 

 wherefis a link function.
But what function shouldfbe? A link function’s job is to map the linear space of a model likeα+β(xi− ̄x)onto the non-linear space of a parameter likeθ. Sofis chosen with that goal in mind. Most of the time, for most GLMs, you can use one of two exceedingly common links, alogit linkor alog link. Let’s introduce each, and you’ll work with both in later chapters.
The **logit link** maps a parameter that is defined as a probability mass, and therefore constrained to lie between zero and one, onto a linear model that can take on any real value.
This link is extremely common when working with binomial GLMs. In the context of a model definition, it looks like this: 

 And the logit function itself is defined as thelog-odds: 

 The “odds” of an event are just the probability it happens divided by the probability it does not happen. So really all that is being stated here is: 

 

 Figure 10.7. The logit link transforms a linear model (left) into a probability (right). This transformation compresses the geometry far from zero, such that a unit change on the linear scale (left) means less and less change on the probability scale (right).

 The above function is usually called the **logistic**. In this context, it is also commonly called the **inverse-logit** , because it inverts the logit transform.
What all of this means is that when you use a logit link for a parameter, you are defining the parameter’s value to be the logistic transform of the linear model.Figure 10.7illustrates the transformation that takes place when using a logit link. On the left, the geometry of the linear model is shown, with horizontal lines indicating unit changes in the value of the linear model as the value of a predictorxchanges. This is the log-odds space, which extends continuously in both positive and negative directions. On the right, the linear space is transformed and is now constrained entirely between zero and one. The horizontal lines have been compressed near the boundaries, in order to make the linear space fit within the probability space. This compression produces the characteristic logistic shape of the transformed linear model shown in the right-hand plot.
This compression does affect interpretation of parameter estimates, because no longer does a unit change in a predictor variable produce a constant change in the mean of the outcome variable. Instead, a unit change inximay produce a larger or smaller change in the probabilitypi, depending upon how far from zero the log-odds are. For example, in Figure 10.7, whenx=0 the linear model has a value of zero on the log-odds scale. A halfunit increase inxresults in about a 0.25 increase in probability. But each addition half-unit will produce less and less of an increase in probability, until any increase is vanishingly small.
And if you think about it, a good model of probability needs to behave this way. When an 
  

 Figure 10.8. The log link transforms a linear model (left) into a strictly positive measurement (right). This transform results in an exponential scaling of the linear model, with a unit change on the linear scale mapping onto increasingly larger changes on the outcome scale.
``` event is almost guaranteed to happen, its probability cannot increase very much, no matter how important the predictor may be.
You’ll find examples of this compression phenomenon in later chapters. The key lesson for now is just that no regression coefficient, such asβ, from a GLM ever produces a constant change on the outcome scale. Recall that we defined interaction (Chapter 8) as a situation in which the effect of a predictor depends upon the value of another predictor. Well now every predictor essentially interacts with itself, because the impact of a change in a predictor depends upon the value of the predictor before the change. More generally, every predictor variable effectively interacts with every other predictor variable, whether you explicitly model them as interactions or not. This fact makes the visualization of counter-factual predictions even more important for understanding what the model is telling you.
The second very common link function is the **log link**. This link function maps a parameter that is defined over only positive real values onto a linear model. For example, suppose we want to model the standard deviationσof a Gaussian distribution so it is a function of a predictor variablex. The parameterσmust be positive, because a standard deviation cannot be negative nor can it be zero. The model might look like: 

 In this model, the meanμis constant, but the standard deviation scales with the valuexi.
A log link is both conventional and useful in this situation. It preventsσfrom taking on a negative value.
What the log link effectively assumes is that the parameter’s value is the exponentiation of the linear model. Solving log(σi)=α+βxiforσiyields the inverse link: 

 

 The impact of this assumption can be seen inFigure 10.8. Using a log link for a linear model (left) implies an exponential scaling of the outcome with the predictor variable (right).
Another way to think of this relationship is to remember that logarithms aremagnitudes. An increase of one unit on the log scale means an increase of an order of magnitude on the untransformed scale. And this fact is reflected in the widening intervals between the horizontal lines in the right-hand plot ofFigure 10.8.
While using a log link does solve the problem of constraining the parameter to be positive, it may also create a problem when the model is asked to predict well outside the range of data used to fit it. Exponential relationships grow, well, exponentially. Just like a linear model cannot be linear forever, an exponential model cannot be exponential forever.
Human height cannot be linearly related to weight forever, because very heavy people stop getting taller and start getting wider. Likewise, the property damage caused by a hurricane may be approximately exponentially related to wind speed for smaller storms. But for very big storms, damage may be capped by the fact that everything gets destroyed.
 ``` Rethinking: When in doubt, play with assumptions. Link functions are assumptions. And like all assumptions, they are useful in different contexts. The conventional logit and log links are widely useful, but they can sometimes distort inference. If you ever have doubts, and want to reassure yourself that your conclusions are not sensitive to choice of link function, then you can use sensitivity analysis. A sensitivity analysis explores how changes in assumptions influence inference. If none of the alternative assumptions you consider have much impact on inference, that’s worth reporting.
Likewise, if the alternatives you consider do have an important impact on inference, that’s also worth reporting. The same sort of advice follows for other modeling assumptions: likelihoods, linear models, priors, and even how the model is fit to data.
Some people are nervous about sensitivity analysis, because it feels like fishing for results, or “phacking.”^172 The goal of sensitivity analysis is really the opposite ofp-hacking. Inp-hacking, many justifiable analyses are tried, and the one that attains statistical significance is reported. In sensitivity analysis, many justifiable analyses are tried, and all of them are described.

 Overthinking: Parameters interacting with themselves. We can find some clarity on how GLMs force every predictor variable to interact with itself by deriving the rate of change in the outcome for a given change in the value of the predictor. In a classic Gaussian model the mean is modeled as μ=α+βx. So the rate of change inμwith respect toxis just∂μ/∂x=β. And that’s constant.
It doesn’t matter what valuexhas. Now consider the rate of change in a binomial probabilitypwith respect tox. The probabilitypis defined by: 
 exp(α+βx) 1 +exp(α+βx) And now taking the derivative with respect toxyields: ∂p ∂x 

 
1 +cosh(α+βx) 
 Sincexappears in this answer, the impact of a change inxdepends uponx. That’s an interaction with itself. The rate of change in the odds is a little nicer: ∂p/( 1 −p) ∂x =βexp(α+βx) but it still contains the entire linear model. Sometimes people avoid non-linear models because they don’t like having to interpret non-linear effects. But if the actual phenomenon contains nonlinearities, this solves only a small world problem.
``` 
 
**10.2.3. Omitted variable bias again.** Back inChapters 5and6, you saw some examples of **omitted variable bias** , where leaving a causally important variable out of a model leads to biased inference. The same thing can of course happen in GLMs. But it can be worse in GLMs, because even a variable that isn’t technically a confounder can bias inference, once we have a link function. The reason is that the ceiling and floor effects described above can distort estimates by suppressing the causal influence of a variable.
Suppose for example that two variablesXandZindependently influence a binary outcomeY. If eitherXandZis large enough, thenY=1. Both variables are sufficient causes ofY. Now if we don’t measureZbut onlyX, we might consistently underestimate the causal effect ofX. Why? BecauseZis sufficient forYto equal 1, and we didn’t measureZ. So there are cases in the data whereXis small butY=1. These cases implyXdoes not influenceY very strongly, but only because we are ignoringZ. This phenomenon doesn’t occur in ordinary linear regression, because independent causes just contribute to the mean. There are no ceiling or floor effects (in theory).
There is no avoiding this problem. Falling back on a linear, rather than generalized linear, model won’t change the reality of omitted variable bias. It will just statistically disguise it.
That may be a good publication strategy, but it’s not a good inferential strategy.
 **10.2.4. Absolute and relative differences.** There is an important practical consequence of the way that a link function compresses and expands different portions of the linear model’s range: Parameter estimates do not by themselves tell you the importance of a predictor on the outcome. The reason is that each parameter represents arelativedifference on the scale of the linear model, ignoring other parameters, while we are really interested inabsolute differences in outcomes that must incorporate all parameters.
This point will come up again in the context of data examples in later chapters, when it will be easier to illustrate its importance. For now, just keep in mind that a big beta-coefficient may not correspond to a big effect on the outcome.
 **10.2.5. GLMs and information criteria.** What you learned inChapter 7about information criteria and regularizing priors applies also to GLMs. But with all these new outcome distributions at your command, it is tempting to use information criteria to compare models with different likelihood functions. Is a Gaussian or binomial better? Can’t we just let WAIC or cross-validation sort it out? Unfortunately, WAIC (or any other predictive criterion) cannot sort it out. The problem is that deviance is part normalizing constant. The constant affects the absolute magnitude of the deviance, but it doesn’t affect fit to data. Since information criteria are all based on deviance, their magnitude also depends upon these constants. That is fine, as long as all of the models you compare use the same outcome distribution type—Gaussian, binomial, exponential, gamma, Poisson, or another. In that case, the constants subtract out when you compare models by their differences. But if two models have different outcome distributions, the constants don’t subtract out, and you can be misled by a difference in AIC/WAIC/PSIS.
Really all you have to remember is to only compare models that all use the same type of likelihood. Of course it is possible to compare models that use different likelihoods, just not with information criteria. Luckily, the principle of maximum entropy ordinarily motivates an easy choice of likelihood, at least for ordinary regression models. So there is no need to lean on model comparison for this modeling choice.
There are a few nuances with WAIC/PSIS and individual GLM types. These nuances will arise as examples of each GLM are worked, in later chapters.
 

 ### 10.3. Maximum entropy priors 
The principle of maximum entropy helps us to make modeling choices. When pressed to choose an outcome distribution—a likelihood—maximum entropy nominates the least informative distribution consistent with the constraints on the outcome variable. Applying the principle in this way leads to many of the same distributional choices that are commonly regarded as just convenient assumptions or useful conventions.
Another way that the principle of maximum entropy helps with choosing distributions arises when choosing priors. GLMs are easy to use with conventional weakly informative priors of the sort you’ve been using up to this point in the book. Such priors are nice, because they allow the data to dominate inference while also taming some of the pathologies of unconstrained estimation. There were some examples of their “soft power” inChapter 9.
But sometimes, rarely, some of the parameters in a GLM refer to things we might actually have background information about. When that’s true, maximum entropy provides a way to generate a prior that embodies the background information, while assuming as little else as possible. This makes them appealing, conservative choices.
We won’t be using maximum entropy to choose priors in this book, but when you come across an analysis that does, you can interpret the principle in the same way as you do with likelihoods and understand the approach as an attempt to include relevant background information about parameters, while introducing no other assumptions by accident.
 ### 10.4. Summary 
This chapter has been a conceptual, not practical, introduction to maximum entropy and generalized linear models. The principle of maximum entropy provides an empirically successful way to choose likelihood functions. Information entropy is essentially a measure of the number of ways a distribution can arise, according to stated assumptions. By choosing the distribution with the biggest information entropy, we thereby choose a distribution that obeys the constraints on outcome variables, without importing additional assumptions.
Generalized linear models arise naturally from this approach, as extensions of the linear models in previous chapters. The necessity of choosing a link function to bind the linear model to the generalized outcome introduces new complexities in model specification, estimation, and interpretation. You’ll become comfortable with these complexities through examples in later chapters. know anything about the physics of cords or knots.
We just have to bet on entropy. Events that can happen vastly more ways are more likely.
Exploiting entropy is not going to untie your cords. But it will help you solve some problems in choosing distributions. Statistical models force many choices upon us. Some of these choices are distributions that represent uncertainty. We must choose, for each parameter, a prior distribution. And we must choose a likelihood function, which serves as a distribution of data. There are conventional choices, such as wide Gaussian priors and the Gaussian likelihood of linear regression. These conventional choices work unreasonably well in many circumstances. But very often the conventional choices are not the best choices. Inference can be more powerful when we use all of the information, and doing so usually requires going beyond convention.
To go beyond convention, it helps to have some principles to guide choice. When an engineer wants to make an unconventional bridge, engineering principles help guide choice.
When a researcher wants to build an unconventional model, entropy provides one useful principle to guide choice of probability distributions: Bet on the distribution with the biggest entropy. Why? There are three sorts of justifications.
First, the distribution with the biggest entropy is the widest and least informative distribution. Choosing the distribution with the largest entropy means spreading probability as evenly as possible, while still remaining consistent with anything we think we know about a process. In the context of choosing a prior, it means choosing the least informative distribution consistent with any partial scientific knowledge we have about a parameter. In the context of choosing a likelihood, it means selecting the distribution we’d get by counting up all the ways outcomes could arise, consistent with the constraints on the outcome variable.
In both cases, the resulting distribution embodies the least information while remaining true to the information we’ve provided.
Second, nature tends to produce empirical distributions that have high entropy. Back in Chapter 4, I introduced the Gaussian distribution by demonstrating how any process that repeatedly adds together fluctuations will tend towards an empirical distribution with the distinctive Gaussian shape. That shape is the one that contains no information about the underlying process except its location and variance. As a result, it has maximum entropy.
299 